Current State of Issue Label Classification

**PROBLEM**

OSS projects rely heavily on users reporting issues with the software so that they can be worked on and developers rely on labels for organization of these issues. The most common labels are bug, feature/enhancement, and question. There are a lot of other labels that can be used, specific to the project itself. For example, one can label “VERSION=5” or “Web Interface” if they are available for the project. Using these labels sheds more light on the problem and makes the job of a developer who is looking for issues to solve easier. Unfortunately for developers, people are lazy when creating issues and usually leave them without tags. This problem leaves us with the need to make a machine learning based issue classification bot.

**RESEARCHED MODELS**

**    **Using ML to make an issue classification model has been an active field of research for over a decade now. Recently, Herbold et al. wrote _[On the Feasibility of Automated Prediction of Bug and Non-Bug Issues](https://arxiv.org/abs/2003.05357)_. They surveyed the current state of the field and tried to introduce some of their own models. They also looked extensively at the cost of mislabelled issues, something that researchers have previously found to be a very common problem in self-reported issues. We will not be concerned with fixing this problem as it is out of scope but it is important to note that a lot of noise exists in the labels made by random people on OSS projects. All the works done thus far have been concerned with classifying issues into major groups: all researched classification models often take the form of bug / not bug, or bug / feature, or bug / feature / question. This is because these tags are the standard tags or derivatives of these tags are used in almost every repository and thus models can be trained and validated on extensive data when only these tags are used. Compared to a tag such as “VERSION=5”, these tags have the same meaning across all repositories. Large amounts of data can be downloaded from [BigQuery](https://console.cloud.google.com/bigquery?p=bigquery-public-data&d=github_repos&page=dataset), where github stores contents from millions of public repositories. This is what most research papers use for their datasets. In Herbold’s survey, it was found that _[Ticket Tagger](https://ieeexplore.ieee.org/document/8918993)_ by Kallis et al. performed about as well or better than all other methods surveyed or introduced in their paper. Ticket Tagger is a tool that uses [fastText](https://fasttext.cc/), an open-source, lightweight deep neural network used for automatic classification of text. The results of Ticket Tagger using 10-fold cross validation of 30,000 issues (10,000 per category) are shown in Figure 1. This seems to be the SOTA in this problem, as the research from Herbold et al. suggests.

**EXISTING CODE**

    Ticket Tagger has an open github [repository](https://github.com/rafaelkallis/ticket-tagger) as well as an implemented github app that automatically tags issues. If one wants to use the fastText model and combine it with a github app, a derivative of this repository is a good option, the only drawback being that it is all coded in JavaScript. Kallis and some colleagues also [expanded](https://github.com/ChristianBirchler/ticket-tagger-analysis) on Ticket Tagger by trying a bunch of different models on the same dataset and including some pre-processing steps, as well as trying it out on a use case of one repository with many issues - [pandas](https://github.com/pandas-dev/pandas). They find that some models actually outperform fastText. They use Python but made no implementation of the models as an app. Using a computationally expensive BERT model, a previous Red Hat intern achieved [good results](https://github.com/thoth-station/Github-Issues-Classifier) classifying bugs, enhancements, and questions with options to run it as an API. There is also a [repo](https://github.com/machine-learning-apps/Issue-Label-Bot) and a [blog article](https://towardsdatascience.com/mlapp-419f90e8f007) about how to deploy an app on github to automate issue classification. The model used was purposefully made to be as simple as possible, and one can replace the model with a better one and have an easy guide on how to deploy the model as a github app.

**CONCLUSIONS & FUTURE WORK**

**    **By looking at previous results, we can conclude there is a high feasibility of implementing a github bot that automatically tags issues as either bugs, enhancements, or questions. What is substantially more difficult is the question of whether this will be possible for other features as well. Clearly this can not be done in a general way in such a way that it can apply to any label in any github repos, and it will have to be done case-by-case for specific labels from specific repositories. The problem that arises from this is the low amount of training data that would be available. Having several thousand issues is rare, and even then it is rare that a specific label that isn’t one of the top labels mentioned will have more than 200 mentions. This is generally not enough data points to train a reliable ML algorithm. However, with a pretrained fastText, it may be possible since those models are able to build on their understanding of language. Future work in this direction will be training the fastText algorithm to the new issues, as well as applying the other approaches tried out by Kallis and their colleagues and seeing which algorithms perform the best on the small dataset and if any can be reliably used in practice.
